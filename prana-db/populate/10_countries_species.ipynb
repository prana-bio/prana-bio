{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get species nile data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "import os\n",
    "import uuid\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Connection parameters\n",
    "dbname = os.getenv(\"DB_NAME\")\n",
    "user = os.getenv(\"DB_USER\")\n",
    "password = os.getenv(\"DB_PASSWORD\")\n",
    "host = os.getenv(\"DB_HOST\")\n",
    "port = os.getenv(\"DB_PORT\")\n",
    "\n",
    "# Connect to the database\n",
    "conn = psycopg2.connect(dbname=dbname, user=user, password=password, host=host, port=port)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Fetch data in chunks and append to a dictionary\n",
    "chunk_size = 100\n",
    "offset = 0\n",
    "species_dict = {}\n",
    "while True:\n",
    "    cursor.execute(\"SELECT species_id, binominal_name FROM species ORDER BY iucn_taxon_id DESC LIMIT %s OFFSET %s\", (chunk_size, offset))\n",
    "    data = cursor.fetchall()\n",
    "    if not data:\n",
    "        break\n",
    "    for record in data:\n",
    "        species_dict[record[0]] = record[1]\n",
    "    offset += chunk_size\n",
    "    logger.info(f\"Fetched {offset} records\")\n",
    "\n",
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "# Print the dictionary\n",
    "print(species_dict)\n",
    "\n",
    "# Convert dictionary to DataFrame\n",
    "df = pd.DataFrame(list(species_dict.items()), columns=['species_id', 'binominal_name'])\n",
    "\n",
    "# Write DataFrame to CSV\n",
    "df.to_csv('species_nile.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get country nile data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "import os\n",
    "import uuid\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Connection parameters\n",
    "dbname = os.getenv(\"DB_NAME\")\n",
    "user = os.getenv(\"DB_USER\")\n",
    "password = os.getenv(\"DB_PASSWORD\")\n",
    "host = os.getenv(\"DB_HOST\")\n",
    "port = os.getenv(\"DB_PORT\")\n",
    "\n",
    "# Connect to the database\n",
    "conn = psycopg2.connect(dbname=dbname, user=user, password=password, host=host, port=port)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "countries_dict = {}\n",
    "\n",
    "cursor.execute(\"SELECT country_id, iso_alpha3 FROM countries\")\n",
    "\n",
    "data = cursor.fetchall()\n",
    "for record in data:\n",
    "   countries_dict[record[0]] = record[1]\n",
    "   logger.info(f\"Fetched country records\")\n",
    "\n",
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "# Print the dictionary\n",
    "print(countries_dict)\n",
    "\n",
    "# Convert dictionary to DataFrame\n",
    "countries_df = pd.DataFrame(list(countries_dict.items()), columns=['country_id', 'iso_alpha3'])\n",
    "\n",
    "# Write DataFrame to CSV\n",
    "countries_df.to_csv('../data/countries_nile.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "join with countries_species.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\conno\\AppData\\Local\\Temp\\ipykernel_4484\\1433584336.py:5: DtypeWarning: Columns (3,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  countries_species_df = pd.read_csv('../data/countries_species.csv')\n"
     ]
    }
   ],
   "source": [
    "# species nile\n",
    "species_nile_df = pd.read_csv('../data/species_nile.csv')\n",
    "\n",
    "# Load countries_species.csv into a DataFrame\n",
    "countries_species_df = pd.read_csv('../data/countries_species.csv')\n",
    "\n",
    "# Merge df with countries_species_df on the 'binominal_name' column\n",
    "merged_df = pd.merge(species_nile_df, countries_species_df, left_on='binominal_name', right_on='binomial', how='inner')\n",
    "\n",
    "merged_df = pd.merge(merged_df, countries_df, left_on='iso_a3', right_on='iso_alpha3', how='inner')\n",
    "\n",
    "merged_df.to_csv('../data/merged_countries_species.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write to temp files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:54 temporary files created with chunks of data.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Define chunk size\n",
    "CHUNK_SIZE = 10000\n",
    "\n",
    "# Function to create temporary files with chunks of data\n",
    "def create_temp_files(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    file_dir = '../data/species_countries/temp'\n",
    "    if not os.path.exists(file_dir):\n",
    "        os.makedirs(file_dir)\n",
    "    num_chunks = len(df) // CHUNK_SIZE + 1\n",
    "    for i in range(num_chunks):\n",
    "        chunk_df = df[i*CHUNK_SIZE:(i+1)*CHUNK_SIZE]\n",
    "        chunk_file = os.path.join(file_dir, f'temp_chunk_{i}.csv')\n",
    "        chunk_df.to_csv(chunk_file, index=False)\n",
    "    return num_chunks\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "csv_file = '../data/merged_countries_species.csv'\n",
    "\n",
    "# Create temporary files with chunks of data\n",
    "num_chunks = create_temp_files(csv_file)\n",
    "logger.info(f\"{num_chunks} temporary files created with chunks of data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parrellel process across files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import extras, pool\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Load environment variables and return database connection parameters\n",
    "def load_db_params():\n",
    "    load_dotenv()\n",
    "    return {\n",
    "        \"dbname\": os.getenv(\"DB_NAME\"),\n",
    "        \"user\": os.getenv(\"DB_USER\"),\n",
    "        \"password\": os.getenv(\"DB_PASSWORD\"),\n",
    "        \"host\": os.getenv(\"DB_HOST\"),\n",
    "        \"port\": os.getenv(\"DB_PORT\")\n",
    "    }\n",
    "\n",
    "# Log the name of processed files along with the number of records written\n",
    "def log_processed_file(file_path, num_records_written, log_file_path='processed_files.log'):\n",
    "    with open(log_file_path, 'a') as log_file:\n",
    "        log_file.write(f\"{file_path}: {num_records_written} records written\\n\")\n",
    "\n",
    "# Get the set of already processed files\n",
    "def get_processed_files(log_file_path='processed_files.log'):\n",
    "    try:\n",
    "        with open(log_file_path, 'r') as log_file:\n",
    "            return set(line.strip().split(': ')[0] for line in log_file)\n",
    "    except FileNotFoundError:\n",
    "        return set()\n",
    "\n",
    "# Log failed records to a CSV file\n",
    "def log_failed_records(records, log_file_path='failed_records.csv'):\n",
    "    records.to_csv(log_file_path, index=False, mode='a', header=not os.path.exists(log_file_path))\n",
    "\n",
    "# Process a single chunk of data from a CSV file\n",
    "def process_and_insert_chunk(chunk, db_pool, file_path, failed_records):\n",
    "    conn = db_pool.getconn()\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        values = [\n",
    "            (\n",
    "                str(uuid.uuid4()),  # Generate UUID for country_species_id\n",
    "                row['country_id'],  # Already string\n",
    "                row['species_id'],  # Already string\n",
    "                row['datanam_area'],  # Already rounded\n",
    "                row['datanam_pct_area'],  # Already rounded\n",
    "                time.strftime('%Y-%m-%d %H:%M:%S'),  # Current timestamp\n",
    "                time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            )\n",
    "            for index, row in chunk.iterrows()\n",
    "        ]\n",
    "        extras.execute_values(cursor, \"\"\"\n",
    "            INSERT INTO countries_species (\n",
    "                country_species_id, country_id, species_id, country_habitat_range_area,\n",
    "                country_habitat_range_area_pct, created, updated\n",
    "            ) VALUES %s;\n",
    "        \"\"\", values)\n",
    "        conn.commit()\n",
    "        return len(chunk)  # Return number of records written\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        failed_records = pd.concat([failed_records, chunk], ignore_index=True)\n",
    "        print(f\"Failed to insert records from {file_path}: {e}\")\n",
    "        return 0  # Return 0 if insertion fails\n",
    "    finally:\n",
    "        db_pool.putconn(conn)\n",
    "\n",
    "# Process an entire file with retry logic\n",
    "def process_file_with_retry(file_path, db_pool, max_file_retries=3, max_chunk_retries=3, wait_seconds=5):\n",
    "    failed_records = pd.DataFrame(columns=['country_id', 'species_id', 'datanam_area', 'datanam_pct_area'])\n",
    "    file_attempts = 0\n",
    "    while file_attempts < max_file_retries:\n",
    "        try:\n",
    "            df_data = pd.read_csv(file_path)  # Read entire file\n",
    "            df_data['country_id'] = df_data['country_id'].astype(str)  # Convert to string\n",
    "            df_data['species_id'] = df_data['species_id'].astype(str)  # Convert to string\n",
    "            df_data['datanam_area'] = df_data['datanam_area'].round(2)  # Limit to 2 decimal places\n",
    "            df_data['datanam_pct_area'] = df_data['datanam_pct_area'].round(2)  # Limit to 2 decimal places\n",
    "            chunks = [df_data[i:i+100] for i in range(0, len(df_data), 100)]  # Split data into chunks of 100 rows\n",
    "            num_records_written = 0\n",
    "            for chunk in chunks:\n",
    "                num_records_written += process_and_insert_chunk(chunk, db_pool, file_path, failed_records)\n",
    "            if num_records_written > 0:\n",
    "                log_processed_file(str(file_path), num_records_written)  # Log the file as processed if records are written\n",
    "                if not failed_records.empty:\n",
    "                    log_failed_records(failed_records)\n",
    "                print(f\"{file_path} processed successfully with retries. {num_records_written} records written.\")\n",
    "            else:\n",
    "                print(f\"No records written for {file_path}. Retrying...\")\n",
    "                time.sleep(wait_seconds)  # Wait before retrying the file\n",
    "            break  # Break the file retry loop on success\n",
    "        except Exception as file_error:\n",
    "            print(f\"Error processing file {file_path}, attempt {file_attempts+1}: {file_error}\")\n",
    "            file_attempts += 1\n",
    "            time.sleep(wait_seconds)  # Wait before retrying the file\n",
    "            if file_attempts == max_file_retries:\n",
    "                print(f\"Failed to process {file_path} after {max_file_retries} attempts.\")\n",
    "\n",
    "# Parallel file processing with file and batch retries\n",
    "if __name__ == \"__main__\":\n",
    "    db_params = load_db_params()\n",
    "    db_pool = psycopg2.pool.SimpleConnectionPool(1, 20, **db_params)\n",
    "\n",
    "    data_directory = Path('../data/species_countries/temp')\n",
    "    csv_files = [file for file in data_directory.glob('*.csv')]\n",
    "    processed_files = get_processed_files()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = {executor.submit(process_file_with_retry, file_path, db_pool): file_path for file_path in csv_files if str(file_path) not in processed_files}\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            file_path = futures[future]\n",
    "\n",
    "    db_pool.closeall()\n",
    "    print(\"Processing complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biodiversity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
