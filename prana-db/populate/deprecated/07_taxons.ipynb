{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach 1: Row by Row (working, but slow, throughput is about 600 records per minute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import uuid\n",
    "import pandas as pd\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Database connection parameters\n",
    "dbname = os.getenv(\"DB_NAME\")\n",
    "user = os.getenv(\"DB_USER\")\n",
    "password = os.getenv(\"DB_PASSWORD\")\n",
    "host = os.getenv(\"DB_HOST\")\n",
    "port = os.getenv(\"DB_PORT\")\n",
    "\n",
    "# Path to your taxons TSV file\n",
    "tsv_file_path = '../data/taxons.tsv'  # Update to the correct path\n",
    "\n",
    "# Read TSV file into DataFrame with chunking\n",
    "df_taxons = pd.read_csv(tsv_file_path, sep='\\t', chunksize=100)  # Adjust chunk size as needed\n",
    "\n",
    "# Establishing Database Connection\n",
    "try:\n",
    "    conn = psycopg2.connect(dbname=dbname, user=user, password=password, host=host, port=port)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    for chunk in df_taxons:\n",
    "        for index, row in chunk.iterrows():\n",
    "            # Generate UUID for each taxon\n",
    "            taxon_id = uuid.uuid4()\n",
    "\n",
    "            # Prepare the SQL INSERT statement with all columns\n",
    "            sql_statement = f\"\"\"\n",
    "            INSERT INTO taxons (\n",
    "                taxon_id, scientific_name, canonical_name, generic_name, taxonomic_status,\n",
    "                kingdom, phylum, class, \"order\", family, genus\n",
    "            )\n",
    "            VALUES (\n",
    "                '{taxon_id}',\n",
    "                '{row['scientificName'].replace(\"'\", \"''\") if pd.notna(row['scientificName']) else ''}',\n",
    "                '{row['canonicalName'].replace(\"'\", \"''\") if pd.notna(row['canonicalName']) else ''}',\n",
    "                '{row['genericName'].replace(\"'\", \"''\") if pd.notna(row['genericName']) else ''}',\n",
    "                '{row['taxonomicStatus'].replace(\"'\", \"''\") if pd.notna(row['taxonomicStatus']) else ''}',\n",
    "                '{row['kingdom'].replace(\"'\", \"''\") if pd.notna(row['kingdom']) else ''}',\n",
    "                '{row['phylum'].replace(\"'\", \"''\") if pd.notna(row['phylum']) else ''}',\n",
    "                '{row['class'].replace(\"'\", \"''\") if pd.notna(row['class']) else ''}',\n",
    "                '{row['order'].replace(\"'\", \"''\") if pd.notna(row['order']) else ''}',\n",
    "                '{row['family'].replace(\"'\", \"''\") if pd.notna(row['family']) else ''}',\n",
    "                '{row['genus'].replace(\"'\", \"''\") if pd.notna(row['genus']) else ''}'\n",
    "            );\n",
    "            \"\"\"\n",
    "\n",
    "            # Execute the SQL statement\n",
    "            cursor.execute(sql_statement)\n",
    "\n",
    "        # Commit after each chunk\n",
    "        conn.commit()\n",
    "        print(\"Chunk inserted successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "finally:\n",
    "    # Close the cursor and connection to clean up\n",
    "    if conn:\n",
    "        cursor.close()\n",
    "        conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Parallelization, DB Pool w/o Retries (1 file)\n",
    "\n",
    "- 2 mil (before error handle line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import pool, extras\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Database connection parameters\n",
    "db_params = {\n",
    "    \"dbname\": os.getenv(\"DB_NAME\"),\n",
    "    \"user\": os.getenv(\"DB_USER\"),\n",
    "    \"password\": os.getenv(\"DB_PASSWORD\"),\n",
    "    \"host\": os.getenv(\"DB_HOST\"),\n",
    "    \"port\": os.getenv(\"DB_PORT\")\n",
    "}\n",
    "\n",
    "# Initialize the connection pool\n",
    "db_pool = psycopg2.pool.SimpleConnectionPool(1, 10, **db_params)\n",
    "\n",
    "def truncate(value, length):\n",
    "    \"\"\"Truncate the string to a specified length.\"\"\"\n",
    "    return (value[:length] if len(value) > length else value) if value else ''\n",
    "\n",
    "\n",
    "def process_and_insert_chunk(chunk):\n",
    "    conn = db_pool.getconn()  # Get a connection from the pool\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Prepare the values for insertion\n",
    "        values = []\n",
    "        for index, row in chunk.iterrows():\n",
    "            taxon_id = str(uuid.uuid4())  # Convert UUID to string\n",
    "            values.append((\n",
    "                taxon_id,\n",
    "                truncate(row['scientificName'] if pd.notnull(row['scientificName']) else '', 255),\n",
    "                truncate(row['canonicalName'] if pd.notnull(row['canonicalName']) else '', 255),\n",
    "                truncate(row['genericName'] if pd.notnull(row['genericName']) else '', 255),\n",
    "                truncate(row['taxonomicStatus'] if pd.notnull(row['taxonomicStatus']) else '', 50),\n",
    "                truncate(row['kingdom'] if pd.notnull(row['kingdom']) else '', 100),\n",
    "                truncate(row['phylum'] if pd.notnull(row['phylum']) else '', 100),\n",
    "                truncate(row['class'] if pd.notnull(row['class']) else '', 100),\n",
    "                truncate(row['order'] if pd.notnull(row['order']) else '', 100),\n",
    "                truncate(row['family'] if pd.notnull(row['family']) else '', 100),\n",
    "                truncate(row['genus'] if pd.notnull(row['genus']) else '', 100)\n",
    "            ))\n",
    "\n",
    "        # Attempt to insert the chunk as a batch\n",
    "        sql_statement = \"\"\"\n",
    "        INSERT INTO taxons (\n",
    "            taxon_id, scientific_name, canonical_name, generic_name, taxonomic_status,\n",
    "            kingdom, phylum, class, \"order\", family, genus\n",
    "        ) VALUES %s;\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            extras.execute_values(cursor, sql_statement, values, template=None, page_size=100)\n",
    "            conn.commit()\n",
    "        except Exception as batch_error:\n",
    "            print(f\"Batch insert failed, attempting row-by-row insertion. Error: {batch_error}\")\n",
    "            conn.rollback()\n",
    "            # Fallback to row-by-row insertion if batch fails\n",
    "            for value in values:\n",
    "                try:\n",
    "                    cursor.execute(sql_statement, (value,))\n",
    "                    conn.commit()\n",
    "                except Exception as row_error:\n",
    "                    print(f\"Error inserting row: {row_error}\")\n",
    "                    conn.rollback()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing chunk: {e}\")\n",
    "        conn.rollback()\n",
    "    finally:\n",
    "        db_pool.putconn(conn)  # Return the connection to the pool\n",
    "\n",
    "# Path to your taxons TSV file and read it into DataFrame with chunking\n",
    "tsv_file_path = '../data/taxons.tsv'\n",
    "df_taxons = pd.read_csv(tsv_file_path, sep='\\t', chunksize=100, error_bad_lines=False, warn_bad_lines=True)\n",
    "\n",
    "# Use ThreadPoolExecutor to parallelize the insertion\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    futures = [executor.submit(process_and_insert_chunk, chunk) for chunk in df_taxons]\n",
    "\n",
    "    for future in as_completed(futures):\n",
    "        print(\"Chunk processed successfully\")\n",
    "\n",
    "# Cleanup: Close the connection pool when done\n",
    "db_pool.closeall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File Parallelization, DB Pool, w/ File and Batch Retry Mechanisms (700 files). Worked!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import extras, pool\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Load environment variables and return database connection parameters\n",
    "def load_db_params():\n",
    "    load_dotenv()\n",
    "    return {\n",
    "        \"dbname\": os.getenv(\"DB_NAME\"),\n",
    "        \"user\": os.getenv(\"DB_USER\"),\n",
    "        \"password\": os.getenv(\"DB_PASSWORD\"),\n",
    "        \"host\": os.getenv(\"DB_HOST\"),\n",
    "        \"port\": os.getenv(\"DB_PORT\")\n",
    "    }\n",
    "\n",
    "# Truncate string fields to their maximum lengths\n",
    "def truncate(value, length):\n",
    "    return (value[:length] if len(value) > length else value) if value else ''\n",
    "\n",
    "# Log the name of processed files\n",
    "def log_processed_file(file_path, log_file_path='processed_files.log'):\n",
    "    with open(log_file_path, 'a') as log_file:\n",
    "        log_file.write(file_path + '\\n')\n",
    "\n",
    "# Get the set of already processed files\n",
    "def get_processed_files(log_file_path='processed_files.log'):\n",
    "    try:\n",
    "        with open(log_file_path, 'r') as log_file:\n",
    "            return set(line.strip() for line in log_file)\n",
    "    except FileNotFoundError:\n",
    "        return set()\n",
    "\n",
    "# Process a single chunk of data from a CSV file\n",
    "def process_and_insert_chunk(chunk, db_pool):\n",
    "    conn = db_pool.getconn()\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        values = [\n",
    "            (\n",
    "                str(uuid.uuid4()),\n",
    "                truncate(row['scientificName'] if pd.notnull(row['scientificName']) else '', 255),\n",
    "                truncate(row['canonicalName'] if pd.notnull(row['canonicalName']) else '', 255),\n",
    "                truncate(row['genericName'] if pd.notnull(row['genericName']) else '', 255),\n",
    "                truncate(row['taxonomicStatus'] if pd.notnull(row['taxonomicStatus']) else '', 50),\n",
    "                truncate(row['kingdom'] if pd.notnull(row['kingdom']) else '', 100),\n",
    "                truncate(row['phylum'] if pd.notnull(row['phylum']) else '', 100),\n",
    "                truncate(row['class'] if pd.notnull(row['class']) else '', 100),\n",
    "                truncate(row['order'] if pd.notnull(row['order']) else '', 100),\n",
    "                truncate(row['family'] if pd.notnull(row['family']) else '', 100),\n",
    "                truncate(row['genus'] if pd.notnull(row['genus']) else '', 100)\n",
    "            )\n",
    "            for index, row in chunk.iterrows()\n",
    "        ]\n",
    "        extras.execute_values(cursor, \"\"\"\n",
    "            INSERT INTO taxons (\n",
    "                taxon_id, scientific_name, canonical_name, generic_name, taxonomic_status,\n",
    "                kingdom, phylum, class, \"order\", family, genus\n",
    "            ) VALUES %s;\n",
    "        \"\"\", values)\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        raise e\n",
    "    finally:\n",
    "        db_pool.putconn(conn)\n",
    "\n",
    "# Process an entire file with retry logic\n",
    "def process_file_with_batch_retry(file_path, db_pool, max_file_retries=3, max_chunk_retries=3, wait_seconds=5):\n",
    "    file_attempts = 0\n",
    "    while file_attempts < max_file_retries:\n",
    "        try:\n",
    "            df_taxons = pd.read_csv(file_path, sep=',', chunksize=100, on_bad_lines='skip')\n",
    "            for chunk in df_taxons:\n",
    "                chunk_attempts = 0\n",
    "                while chunk_attempts < max_chunk_retries:\n",
    "                    try:\n",
    "                        process_and_insert_chunk(chunk, db_pool)\n",
    "                        break  # Break the chunk retry loop on success\n",
    "                    except Exception as chunk_error:\n",
    "                        print(f\"Error processing chunk in {file_path}, attempt {chunk_attempts+1}: {chunk_error}\")\n",
    "                        chunk_attempts += 1\n",
    "                        time.sleep(wait_seconds)  # Wait before retrying the chunk\n",
    "                        if chunk_attempts == max_chunk_retries:\n",
    "                            print(f\"Max chunk retries reached for a chunk in {file_path}.\")\n",
    "\n",
    "            log_processed_file(str(file_path))  # Log the file as processed after all chunks are attempted\n",
    "            print(f\"{file_path} processed successfully with retries.\")\n",
    "            break  # Break the file retry loop on success\n",
    "        except Exception as file_error:\n",
    "            print(f\"Error processing file {file_path}, attempt {file_attempts+1}: {file_error}\")\n",
    "            file_attempts += 1\n",
    "            time.sleep(wait_seconds)  # Wait before retrying the file\n",
    "            if file_attempts == max_file_retries:\n",
    "                print(f\"Failed to process {file_path} after {max_file_retries} attempts.\")\n",
    "\n",
    "# Parallel file processing with file and batch retries\n",
    "if __name__ == \"__main__\":\n",
    "    db_params = load_db_params()\n",
    "    db_pool = psycopg2.pool.SimpleConnectionPool(1, 20, **db_params)\n",
    "\n",
    "    data_directory = Path('../data/temp')\n",
    "    csv_files = [file for file in data_directory.glob('taxons_chunk_*.csv')]\n",
    "    processed_files = get_processed_files()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = {executor.submit(process_file_with_batch_retry, file_path, db_pool): file_path for file_path in csv_files if str(file_path) not in processed_files}\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            file_path = futures[future]\n",
    "\n",
    "    db_pool.closeall()\n",
    "    print(\"Processing complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biodiversity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
